{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d679f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f7e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary LangChain components\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.document_loaders import CSVLoader, PyPDFLoader, Docx2txtLoader, DirectoryLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.router import MultiRetrievalQAChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_retrieval_prompt import MULTI_RETRIEVAL_ROUTER_TEMPLATE\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import glob\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512b5179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\91984\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # for cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fcd66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "Sample embedding values: [0.04652441293001175, 0.0034151612780988216, -0.014530838467180729, -0.033341288566589355, 0.03532649949193001]\n"
     ]
    }
   ],
   "source": [
    "# Test the embeddings to make sure they work\n",
    "test_text = \"Hello World, how are you?\"\n",
    "test_embedding = embeddings.embed_query(test_text)\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"Sample embedding values: {test_embedding[:5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fdab808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM (used for routing and query answering)\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.0,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    model=\"openai/gpt-oss-20b:free\",\n",
    ")\n",
    "print(\"LLM initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2db3395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF: .\\iphone17.pdf\n",
      "Loading DOCX: .\\f1info.docx\n",
      "Loading CSV: .\\sales.csv\n",
      "\n",
      "=== Document Loading Summary ===\n",
      "Total documents: 307\n"
     ]
    }
   ],
   "source": [
    "# Document Loader Function - Handles PDF, DOCX, and CSV files\n",
    "def load_documents_by_type(directory: str = \".\") -> Dict[str, List[Document]]:\n",
    "    \"\"\"\n",
    "    Load all documents from directory, organized by type\n",
    "    Returns a dictionary with keys: 'pdf', 'docx', 'csv'\n",
    "    \"\"\"\n",
    "    documents_by_type = {\n",
    "        'pdf': [],\n",
    "        'docx': [],\n",
    "        'csv': []\n",
    "    }\n",
    "    \n",
    "    # Load PDF files\n",
    "    pdf_files = glob.glob(f\"{directory}/*.pdf\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Loading PDF: {pdf_file}\")\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        docs = loader.load()\n",
    "        # Add document type to metadata\n",
    "        for doc in docs:\n",
    "            doc.metadata['doc_type'] = 'pdf'\n",
    "        documents_by_type['pdf'].extend(docs)\n",
    "    \n",
    "    # Load DOCX files\n",
    "    docx_files = glob.glob(f\"{directory}/*.docx\")\n",
    "    for docx_file in docx_files:\n",
    "        print(f\"Loading DOCX: {docx_file}\")\n",
    "        loader = Docx2txtLoader(docx_file)\n",
    "        docs = loader.load()\n",
    "        # Add document type to metadata\n",
    "        for doc in docs:\n",
    "            doc.metadata['doc_type'] = 'docx'\n",
    "        documents_by_type['docx'].extend(docs)\n",
    "    \n",
    "    # Load CSV files\n",
    "    csv_files = glob.glob(f\"{directory}/*.csv\")\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Loading CSV: {csv_file}\")\n",
    "        loader = CSVLoader(file_path=csv_file)\n",
    "        docs = loader.load()\n",
    "        # Add document type to metadata\n",
    "        for doc in docs:\n",
    "            doc.metadata['doc_type'] = 'csv'\n",
    "        documents_by_type['csv'].extend(docs)\n",
    "    \n",
    "    return documents_by_type\n",
    "\n",
    "# Load all documents\n",
    "all_documents = load_documents_by_type(\".\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n=== Document Loading Summary ===\")\n",
    "print(f\"Total documents: {sum(len(docs) for docs in all_documents.values())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95338e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PDF vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91984\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DOCX vector store...\n",
      "Creating CSV vector store...\n",
      "\n",
      "=== Vector Stores Created ===\n",
      "Active retrievers: ['pdf', 'docx', 'csv']\n"
     ]
    }
   ],
   "source": [
    "# Create separate vector stores for each document type\n",
    "vector_stores = {}\n",
    "retrievers = {}\n",
    "\n",
    "# Create vector store for PDFs (if any exist)\n",
    "if all_documents['pdf']:\n",
    "    print(\"Creating PDF vector store...\")\n",
    "    vector_stores['pdf'] = DocArrayInMemorySearch.from_documents(\n",
    "        all_documents['pdf'], \n",
    "        embeddings\n",
    "    )\n",
    "    retrievers['pdf'] = vector_stores['pdf'].as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Create vector store for DOCX files (if any exist)\n",
    "if all_documents['docx']:\n",
    "    print(\"Creating DOCX vector store...\")\n",
    "    vector_stores['docx'] = DocArrayInMemorySearch.from_documents(\n",
    "        all_documents['docx'], \n",
    "        embeddings\n",
    "    )\n",
    "    retrievers['docx'] = vector_stores['docx'].as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Create vector store for CSV files (if any exist)\n",
    "if all_documents['csv']:\n",
    "    print(\"Creating CSV vector store...\")\n",
    "    vector_stores['csv'] = DocArrayInMemorySearch.from_documents(\n",
    "        all_documents['csv'], \n",
    "        embeddings\n",
    "    )\n",
    "    retrievers['csv'] = vector_stores['csv'].as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "print(f\"\\n=== Vector Stores Created ===\")\n",
    "print(f\"Active retrievers: {list(retrievers.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45e3f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap QA chains to accept {\"input\": ...} and forward as {\"query\": ...}\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "wrapped_destinations = {}\n",
    "for name, chain in qa_chains.items():\n",
    "    wrapped_destinations[name] = RunnableLambda(\n",
    "        lambda x, _chain=chain: _chain.invoke({\"query\": x.get(\"input\", x)})\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5a04447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating QA chain for PDF documents...\n",
      "Creating QA chain for DOCX documents...\n",
      "Creating QA chain for CSV documents...\n",
      "\n",
      "=== QA Chains Created ===\n",
      "Available chains: ['pdf', 'docx', 'csv']\n"
     ]
    }
   ],
   "source": [
    "# Create QA chains for each document type\n",
    "qa_chains = {}\n",
    "\n",
    "for doc_type, retriever in retrievers.items():\n",
    "    print(f\"Creating QA chain for {doc_type.upper()} documents...\")\n",
    "    qa_chains[doc_type] = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        verbose=True,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "print(f\"\\n=== QA Chains Created ===\")\n",
    "print(f\"Available chains: {list(qa_chains.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a36665b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router configured with 3 document types\n",
      "Available document types: ['pdf', 'docx', 'csv']\n"
     ]
    }
   ],
   "source": [
    "# Build Router Chain using LangChain\n",
    "# The router determines which document type to query based on the user's question\n",
    "\n",
    "# Define retriever information for the router\n",
    "retriever_infos = []\n",
    "\n",
    "if 'pdf' in retrievers:\n",
    "    retriever_infos.append({\n",
    "        \"name\": \"pdf\",\n",
    "        \"description\": \"The pdf document is all about iphone, iphone 17 series launch.Good for answering questions about PDF documents, reports, articles, papers, documentation, manuals, and textual information stored in PDF format\",\n",
    "        \"retriever\": retrievers['pdf']\n",
    "    })\n",
    "\n",
    "if 'docx' in retrievers:\n",
    "    retriever_infos.append({\n",
    "        \"name\": \"docx\",\n",
    "        \"description\": \"The word document is all about F1 Singapore Grand Prix 2025.Good for answering questions about Word documents, letters, memos, proposals, written content, formatted documents, and business documents stored in DOCX format\",\n",
    "        \"retriever\": retrievers['docx']\n",
    "    })\n",
    "\n",
    "if 'csv' in retrievers:\n",
    "    retriever_infos.append({\n",
    "        \"name\": \"csv\",\n",
    "        \"description\": \"The csv document is all about sales data of a company.Good for answering questions about tabular data, spreadsheets, sales records, orders, numerical data, statistics, transactions, customer data, and structured data stored in CSV format\",\n",
    "        \"retriever\": retrievers['csv']\n",
    "    })\n",
    "\n",
    "print(f\"Router configured with {len(retriever_infos)} document types\")\n",
    "print(f\"Available document types: {[info['name'] for info in retriever_infos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "951d8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router chain created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a custom routing function using LangChain\n",
    "from langchain.chains.router.base import MultiRouteChain\n",
    "from langchain.chains.router.llm_router import RouterChain, RouterOutputParser\n",
    "\n",
    "def create_router_chain():\n",
    "    \"\"\"\n",
    "    Creates a router that uses the LLM to determine which document type to query\n",
    "    \"\"\"\n",
    "    # Create destination chains dictionary\n",
    "    destination_chains = {}\n",
    "    for doc_type in qa_chains.keys():\n",
    "        destination_chains[doc_type] = qa_chains[doc_type]\n",
    "    \n",
    "    # Build router prompt\n",
    "    destinations = []\n",
    "    for info in retriever_infos:\n",
    "        destinations.append(f\"{info['name']}: {info['description']}\")\n",
    "    \n",
    "    destinations_str = \"\\n\".join(destinations)\n",
    "    \n",
    "    router_template = f\"\"\"Given a raw text input to a language model, select the model prompt best suited for the input.\n",
    "You will be given the names of the available document types and a description of what each type is best suited for.\n",
    "You may also revise the original input if you think that revising it will ultimately lead to a better response.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\\\ name of the document type to use\n",
    "    \"next_inputs\": string \\\\ the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate document types specified below.\n",
    "\n",
    "<< CANDIDATE DOCUMENT TYPES >>\n",
    "{destinations_str}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT >>\n",
    "\"\"\"\n",
    "\n",
    "    router_prompt = PromptTemplate(\n",
    "        template=router_template,\n",
    "        input_variables=[\"input\"],\n",
    "        output_parser=RouterOutputParser(),\n",
    "    )\n",
    "    \n",
    "    router_chain = LLMRouterChain.from_llm(\n",
    "        llm=llm,\n",
    "        prompt=router_prompt,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    return router_chain, destination_chains\n",
    "\n",
    "# Create the router\n",
    "router_chain, destination_chains = create_router_chain()\n",
    "print(\"Router chain created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91da2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router QA chain ready!\n",
      "Available document types: ['pdf', 'docx', 'csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91984\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:253: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    }
   ],
   "source": [
    "# Create a single router QA chain that handles routing and retrieval\n",
    "from langchain.chains.router import MultiRetrievalQAChain\n",
    "\n",
    "router_qa_chain = MultiRetrievalQAChain.from_retrievers(\n",
    "    llm=llm,\n",
    "    retriever_infos=retriever_infos,\n",
    "    default_chain_llm=llm,  # provide default LLM for fallback chain\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"Router QA chain ready!\")\n",
    "print(f\"Available document types: {[info['name'] for info in retriever_infos]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ae8880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MAIN QUERY FUNCTION - PUT YOUR QUESTION HERE\n",
    "# ============================================================\n",
    "\n",
    "def query_documents(user_query: str):\n",
    "    \"\"\"\n",
    "    Main query function - the router automatically determines which document to search\n",
    "    \n",
    "    Args:\n",
    "        user_query: Your question (string)\n",
    "        \n",
    "    The router will analyze your question and route it to:\n",
    "        - PDF documents (for iPhone 17 info)\n",
    "        - DOCX documents (for F1 Singapore GP info)\n",
    "        - CSV files (for sales data analysis)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"USER QUERY: {user_query}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Router automatically picks the right document type\n",
    "        result = router_qa_chain.invoke({\"input\": user_query})\n",
    "        \n",
    "        # Extract answer from result\n",
    "        if isinstance(result, str):\n",
    "            answer = result\n",
    "        elif isinstance(result, dict):\n",
    "            answer = result.get(\"result\") or result.get(\"output\") or str(result)\n",
    "        else:\n",
    "            answer = str(result)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ANSWER:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(answer)\n",
    "        print()\n",
    "        \n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b398298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "USER QUERY: Can you list me five corporate segment orders in the sales data?\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiRetrievalQAChain chain...\u001b[0m\n",
      "csv: {'query': 'Can you list me five corporate segment orders in the sales data?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "============================================================\n",
      "ANSWER:\n",
      "============================================================\n",
      "Here are five orders that belong to the **Corporate** segment from the data you provided:\n",
      "\n",
      "| Order ID | Customer Name | Sales | Product Name |\n",
      "|----------|---------------|-------|--------------|\n",
      "| **US-2017-124303** | Fred Hopkins | $16.06 | Wirebound Message Books, 5‑1/2 × 4 Forms |\n",
      "| **CA-2017-132976** | Andrew Gjertsen | $11.65 | Post‑it “Important Message” Note Pad, Neon Colors |\n",
      "| **US-2017-145366** | Christine Abelman | $57.58 | Recycled Interoffice Envelopes with String and Button Closure |\n",
      "| **US-2014-100853** | Jennifer Braxton | $52.45 | Kensington 7‑Outlet MasterPiece HomeOffice Power Control Center |\n",
      "| **US-2014-156216** | Erin Ashbrook | $18.65 | GBC Instant Index System for Binding Systems |\n",
      "\n",
      "All of these orders are marked as **Corporate** in the segment field.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here are five orders that belong to the **Corporate** segment from the data you provided:\\n\\n| Order ID | Customer Name | Sales | Product Name |\\n|----------|---------------|-------|--------------|\\n| **US-2017-124303** | Fred Hopkins | $16.06 | Wirebound Message Books, 5‑1/2\\u202f×\\u202f4 Forms |\\n| **CA-2017-132976** | Andrew Gjertsen | $11.65 | Post‑it “Important Message” Note Pad, Neon Colors |\\n| **US-2017-145366** | Christine Abelman | $57.58 | Recycled Interoffice Envelopes with String and Button Closure |\\n| **US-2014-100853** | Jennifer Braxton | $52.45 | Kensington 7‑Outlet MasterPiece HomeOffice Power Control Center |\\n| **US-2014-156216** | Erin Ashbrook | $18.65 | GBC Instant Index System for Binding Systems |\\n\\nAll of these orders are marked as **Corporate** in the segment field.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# YOUR CUSTOM QUERY - Edit and run this cell with your question\n",
    "\n",
    "\n",
    "# Put your question here (between the quotes):\n",
    "my_question = \"Can you list me five corporate segment orders in the sales data?\"\n",
    "\n",
    "# Run the query\n",
    "query_documents(my_question)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df5fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Query to Specific Document Type (Advanced Usage)\n",
    "# If you know which document type you want to query, you can bypass the router\n",
    "\n",
    "def query_specific_type(question: str, doc_type: str):\n",
    "    \"\"\"\n",
    "    Query a specific document type directly\n",
    "    doc_type: 'pdf', 'docx', or 'csv'\n",
    "    \"\"\"\n",
    "    if doc_type not in qa_chains:\n",
    "        print(f\"Error: Document type '{doc_type}' not available\")\n",
    "        print(f\"Available types: {list(qa_chains.keys())}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Querying {doc_type.upper()} documents directly...\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        result = qa_chains[doc_type]({\"query\": question})\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ANSWER:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(result['result'])\n",
    "        \n",
    "        # Show source documents\n",
    "        if 'source_documents' in result and result['source_documents']:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"SOURCES ({len(result['source_documents'])} documents):\")\n",
    "            print(f\"{'='*60}\")\n",
    "            for i, doc in enumerate(result['source_documents'][:3], 1):\n",
    "                print(f\"\\nSource {i}:\")\n",
    "                print(f\"File: {doc.metadata.get('source', 'Unknown')}\")\n",
    "                print(f\"Preview: {doc.page_content[:200]}...\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example: Query CSV directly\n",
    "# if 'csv' in qa_chains:\n",
    "#     query_specific_type(\"What is the average sales amount?\", \"csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ca83b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
